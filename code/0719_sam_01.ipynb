{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from typing import Any, Optional, Tuple, Type, List, Dict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54fa6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff3de5-0d0e-497b-ac75-d5179a3f65d3",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838e1d83-8670-407b-82f6-bf9652f58639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76a29e-e9c2-411a-a569-04166f074184",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8496767-2f64-4285-bec4-c6f53a1fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.data.iloc[idx, 2]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "        # print('mask type', mask)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        # print('mask type2', mask)\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc955893-22fd-4320-88be-7aa0d790cbd9",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b708503-2ff9-4584-9d73-40990b3572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = SatelliteDataset(csv_file='../train.csv', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42501fc-b573-4893-a7c4-5e280dfdaf09",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256f471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common.py\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))\n",
    "\n",
    "\n",
    "# From https://github.com/facebookresearch/detectron2/blob/main/detectron2/layers/batch_norm.py # noqa\n",
    "# Itself from https://github.com/facebookresearch/ConvNeXt/blob/d1fa8f6fef0a165b27399986cc2bdacc92777e40/models/convnext.py#L119  # noqa\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ce4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_encoder.py\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "\n",
    "class PromptEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        image_embedding_size: Tuple[int, int],\n",
    "        input_image_size: Tuple[int, int],\n",
    "        mask_in_chans: int,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Encodes prompts for input to SAM's mask decoder.\n",
    "\n",
    "        Arguments:\n",
    "          embed_dim (int): The prompts' embedding dimension\n",
    "          image_embedding_size (tuple(int, int)): The spatial size of the\n",
    "            image embedding, as (H, W).\n",
    "          input_image_size (int): The padded size of the image as input\n",
    "            to the image encoder, as (H, W).\n",
    "          mask_in_chans (int): The number of hidden channels used for\n",
    "            encoding input masks.\n",
    "          activation (nn.Module): The activation to use when encoding\n",
    "            input masks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_image_size = input_image_size\n",
    "        self.image_embedding_size = image_embedding_size\n",
    "        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)\n",
    "\n",
    "        self.num_point_embeddings: int = 4  # pos/neg point + 2 box corners\n",
    "        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)]\n",
    "        self.point_embeddings = nn.ModuleList(point_embeddings)\n",
    "        self.not_a_point_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "        self.mask_input_size = (4 * image_embedding_size[0], 4 * image_embedding_size[1])\n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans // 4),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation(),\n",
    "            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),\n",
    "        )\n",
    "        self.no_mask_embed = nn.Embedding(1, embed_dim)\n",
    "\n",
    "    def get_dense_pe(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the positional encoding used to encode point prompts,\n",
    "        applied to a dense set of points the shape of the image encoding.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: Positional encoding with shape\n",
    "            1x(embed_dim)x(embedding_h)x(embedding_w)\n",
    "        \"\"\"\n",
    "        return self.pe_layer(self.image_embedding_size).unsqueeze(0)\n",
    "\n",
    "    def _embed_points(\n",
    "        self,\n",
    "        points: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        pad: bool,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Embeds point prompts.\"\"\"\n",
    "        points = points + 0.5  # Shift to center of pixel\n",
    "        if pad:\n",
    "            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n",
    "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n",
    "            points = torch.cat([points, padding_point], dim=1)\n",
    "            labels = torch.cat([labels, padding_label], dim=1)\n",
    "        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)\n",
    "        point_embedding[labels == -1] = 0.0\n",
    "        point_embedding[labels == -1] += self.not_a_point_embed.weight\n",
    "        point_embedding[labels == 0] += self.point_embeddings[0].weight\n",
    "        point_embedding[labels == 1] += self.point_embeddings[1].weight\n",
    "        return point_embedding\n",
    "\n",
    "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds box prompts.\"\"\"\n",
    "        boxes = boxes + 0.5  # Shift to center of pixel\n",
    "        coords = boxes.reshape(-1, 2, 2)\n",
    "        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)\n",
    "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n",
    "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n",
    "        return corner_embedding\n",
    "\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Embeds mask inputs.\"\"\"\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Gets the batch size of the output given the batch size of the input prompts.\n",
    "        \"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def _get_device(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "        \"\"\"\n",
    "        bs = self._get_batch_size(points, boxes, masks)\n",
    "        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
    "        if points is not None:\n",
    "            coords, labels = points\n",
    "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "        if boxes is not None:\n",
    "            box_embeddings = self._embed_boxes(boxes)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "\n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else:\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "\n",
    "        return sparse_embeddings, dense_embeddings\n",
    "\n",
    "\n",
    "class PositionEmbeddingRandom(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using random spatial frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_pos_feats: int = 64, scale: Optional[float] = None) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer(\n",
    "            \"positional_encoding_gaussian_matrix\",\n",
    "            scale * torch.randn((2, num_pos_feats)),\n",
    "        )\n",
    "\n",
    "    def _pe_encoding(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n",
    "        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n",
    "        coords = 2 * coords - 1\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix\n",
    "        coords = 2 * np.pi * coords\n",
    "        # outputs d_1 x ... x d_n x C shape\n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1)\n",
    "\n",
    "    def forward(self, size: Tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"Generate positional encoding for a grid of the specified size.\"\"\"\n",
    "        h, w = size\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32)\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5\n",
    "        y_embed = y_embed / h\n",
    "        x_embed = x_embed / w\n",
    "\n",
    "        pe = self._pe_encoding(torch.stack([x_embed, y_embed], dim=-1))\n",
    "        return pe.permute(2, 0, 1)  # C x H x W\n",
    "\n",
    "    def forward_with_coords(\n",
    "        self, coords_input: torch.Tensor, image_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Positionally encode points that are not normalized to [0,1].\"\"\"\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1]\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0]\n",
    "        return self._pe_encoding(coords.to(torch.float))  # B x N x C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce417c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder.py\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\n",
    "class ImageEncoderViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 1024,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        out_chans: int = 256,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_abs_pos: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        global_attn_indexes: Tuple[int, ...] = (),\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int): Input image size.\n",
    "            patch_size (int): Patch size.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "            depth (int): Depth of ViT.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_abs_pos (bool): If True, use absolute positional embeddings.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks.\n",
    "            global_attn_indexes (list): Indexes for blocks using global attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=(patch_size, patch_size),\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.pos_embed: Optional[nn.Parameter] = None\n",
    "        if use_abs_pos:\n",
    "            # Initialize absolute positional embedding with pretrain image size.\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n",
    "            )\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            block = Block(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                use_rel_pos=use_rel_pos,\n",
    "                rel_pos_zero_init=rel_pos_zero_init,\n",
    "                window_size=window_size if i not in global_attn_indexes else 0,\n",
    "                input_size=(img_size // patch_size, img_size // patch_size),\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embed_dim,\n",
    "                out_chans,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "            nn.Conv2d(\n",
    "                out_chans,\n",
    "                out_chans,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.patch_embed(x)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.neck(x.permute(0, 3, 1, 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads in each ViT block.\n",
    "            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n",
    "            norm_layer (nn.Module): Normalization layer.\n",
    "            act_layer (nn.Module): Activation layer.\n",
    "            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            window_size (int): Window size for window attention blocks. If it equals 0, then\n",
    "                use global attention.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            use_rel_pos=use_rel_pos,\n",
    "            rel_pos_zero_init=rel_pos_zero_init,\n",
    "            input_size=input_size if window_size == 0 else (window_size, window_size),\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        # Window partition\n",
    "        if self.window_size > 0:\n",
    "            H, W = x.shape[1], x.shape[2]\n",
    "            x, pad_hw = window_partition(x, self.window_size)\n",
    "\n",
    "        x = self.attn(x)\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n",
    "\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        input_size: Optional[Tuple[int, int]] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Number of input channels.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n",
    "            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n",
    "            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n",
    "            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n",
    "                positional parameter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"Input size must be provided if using relative positional encoding.\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape\n",
    "        # qkv with shape (3, B, nHead, H * W, C)\n",
    "        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v with shape (B * nHead, H * W, C)\n",
    "        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n",
    "\n",
    "        attn = (q * self.scale) @ k.transpose(-2, -1)\n",
    "\n",
    "        if self.use_rel_pos:\n",
    "            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Partition into non-overlapping windows with padding if needed.\n",
    "    Args:\n",
    "        x (tensor): input tokens with [B, H, W, C].\n",
    "        window_size (int): window size.\n",
    "\n",
    "    Returns:\n",
    "        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n",
    "        (Hp, Wp): padded height and width before partition\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows, (Hp, Wp)\n",
    "\n",
    "\n",
    "def window_unpartition(\n",
    "    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Window unpartition into original sequences and removing padding.\n",
    "    Args:\n",
    "        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n",
    "        window_size (int): window size.\n",
    "        pad_hw (Tuple): padded height and width (Hp, Wp).\n",
    "        hw (Tuple): original height and width (H, W) before padding.\n",
    "\n",
    "    Returns:\n",
    "        x: unpartitioned sequences with [B, H, W, C].\n",
    "    \"\"\"\n",
    "    Hp, Wp = pad_hw\n",
    "    H, W = hw\n",
    "    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
    "    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n",
    "\n",
    "    if Hp > H or Wp > W:\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get relative positional embeddings according to the relative positions of\n",
    "        query and key sizes.\n",
    "    Args:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): relative position embeddings (L, C).\n",
    "\n",
    "    Returns:\n",
    "        Extracted positional embeddings according to relative positions.\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]\n",
    "\n",
    "\n",
    "def add_decomposed_rel_pos(\n",
    "    attn: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n",
    "    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n",
    "    Args:\n",
    "        attn (Tensor): attention map.\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        attn (Tensor): attention map with added relative positional embeddings.\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    attn = (\n",
    "        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return attn\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: Tuple[int, int] = (16, 16),\n",
    "        stride: Tuple[int, int] = (16, 16),\n",
    "        padding: Tuple[int, int] = (0, 0),\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel_size (Tuple): kernel size of the projection layer.\n",
    "            stride (Tuple): stride of the projection layer.\n",
    "            padding (Tuple): padding size of the projection layer.\n",
    "            in_chans (int): Number of input image channels.\n",
    "            embed_dim (int): Patch embedding dimension.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.proj(x)\n",
    "        # B C H W -> B H W C\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657bfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_decoder.py\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "class MaskDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        transformer_dim: int,\n",
    "        transformer: nn.Module,\n",
    "        num_multimask_outputs: int = 3,\n",
    "        activation: Type[nn.Module] = nn.GELU,\n",
    "        iou_head_depth: int = 3,\n",
    "        iou_head_hidden_dim: int = 256,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Predicts masks given an image and prompt embeddings, using a\n",
    "        transformer architecture.\n",
    "\n",
    "        Arguments:\n",
    "          transformer_dim (int): the channel dimension of the transformer\n",
    "          transformer (nn.Module): the transformer used to predict masks\n",
    "          num_multimask_outputs (int): the number of masks to predict\n",
    "            when disambiguating masks\n",
    "          activation (nn.Module): the type of activation to use when\n",
    "            upscaling masks\n",
    "          iou_head_depth (int): the depth of the MLP used to predict\n",
    "            mask quality\n",
    "          iou_head_hidden_dim (int): the hidden dimension of the MLP\n",
    "            used to predict mask quality\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer_dim = transformer_dim\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.num_multimask_outputs = num_multimask_outputs\n",
    "\n",
    "        self.iou_token = nn.Embedding(1, transformer_dim)\n",
    "        self.num_mask_tokens = num_multimask_outputs + 1\n",
    "        self.mask_tokens = nn.Embedding(self.num_mask_tokens, transformer_dim)\n",
    "\n",
    "        self.output_upscaling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(transformer_dim, transformer_dim // 4, kernel_size=2, stride=2),\n",
    "            LayerNorm2d(transformer_dim // 4),\n",
    "            activation(),\n",
    "            nn.ConvTranspose2d(transformer_dim // 4, transformer_dim // 8, kernel_size=2, stride=2),\n",
    "            activation(),\n",
    "        )\n",
    "        self.output_hypernetworks_mlps = nn.ModuleList(\n",
    "            [\n",
    "                MLP(transformer_dim, transformer_dim, transformer_dim // 8, 3)\n",
    "                for i in range(self.num_mask_tokens)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.iou_prediction_head = MLP(\n",
    "            transformer_dim, iou_head_hidden_dim, self.num_mask_tokens, iou_head_depth\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "        dense_prompt_embeddings: torch.Tensor,\n",
    "        multimask_output: bool,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predict masks given image and prompt embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          image_embeddings (torch.Tensor): the embeddings from the image encoder\n",
    "          image_pe (torch.Tensor): positional encoding with the shape of image_embeddings\n",
    "          sparse_prompt_embeddings (torch.Tensor): the embeddings of the points and boxes\n",
    "          dense_prompt_embeddings (torch.Tensor): the embeddings of the mask inputs\n",
    "          multimask_output (bool): Whether to return multiple masks or a single\n",
    "            mask.\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: batched predicted masks\n",
    "          torch.Tensor: batched predictions of mask quality\n",
    "        \"\"\"\n",
    "        masks, iou_pred = self.predict_masks(\n",
    "            image_embeddings=image_embeddings,\n",
    "            image_pe=image_pe,\n",
    "            sparse_prompt_embeddings=sparse_prompt_embeddings,\n",
    "            dense_prompt_embeddings=dense_prompt_embeddings,\n",
    "        )\n",
    "\n",
    "        # Select the correct mask or masks for output\n",
    "        if multimask_output:\n",
    "            mask_slice = slice(1, None)\n",
    "        else:\n",
    "            mask_slice = slice(0, 1)\n",
    "        masks = masks[:, mask_slice, :, :]\n",
    "        iou_pred = iou_pred[:, mask_slice]\n",
    "\n",
    "        # Prepare output\n",
    "        return masks, iou_pred\n",
    "\n",
    "    def predict_masks(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        image_pe: torch.Tensor,\n",
    "        sparse_prompt_embeddings: torch.Tensor,\n",
    "        dense_prompt_embeddings: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Predicts masks. See 'forward' for more details.\"\"\"\n",
    "        # Concatenate output tokens\n",
    "        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n",
    "        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)\n",
    "        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)\n",
    "\n",
    "        # Expand per-image data in batch direction to be per-mask\n",
    "        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)\n",
    "        src = src + dense_prompt_embeddings\n",
    "        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)\n",
    "        b, c, h, w = src.shape\n",
    "\n",
    "        # Run the transformer\n",
    "        hs, src = self.transformer(src, pos_src, tokens)\n",
    "        iou_token_out = hs[:, 0, :]\n",
    "        mask_tokens_out = hs[:, 1 : (1 + self.num_mask_tokens), :]\n",
    "\n",
    "        # Upscale mask embeddings and predict masks using the mask tokens\n",
    "        src = src.transpose(1, 2).view(b, c, h, w)\n",
    "        upscaled_embedding = self.output_upscaling(src)\n",
    "        hyper_in_list: List[torch.Tensor] = []\n",
    "        for i in range(self.num_mask_tokens):\n",
    "            hyper_in_list.append(self.output_hypernetworks_mlps[i](mask_tokens_out[:, i, :]))\n",
    "        hyper_in = torch.stack(hyper_in_list, dim=1)\n",
    "        b, c, h, w = upscaled_embedding.shape\n",
    "        masks = (hyper_in @ upscaled_embedding.view(b, c, h * w)).view(b, -1, h, w)\n",
    "\n",
    "        # Generate mask quality predictions\n",
    "        iou_pred = self.iou_prediction_head(iou_token_out)\n",
    "\n",
    "        return masks, iou_pred\n",
    "\n",
    "\n",
    "# Lightly adapted from\n",
    "# https://github.com/facebookresearch/MaskFormer/blob/main/mask_former/modeling/transformer/transformer_predictor.py # noqa\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        num_layers: int,\n",
    "        sigmoid_output: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "        self.sigmoid_output = sigmoid_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        if self.sigmoid_output:\n",
    "            x = F.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9a66615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sam.py\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "class Sam(nn.Module):\n",
    "    mask_threshold: float = 0.0\n",
    "    image_format: str = \"RGB\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_encoder: ImageEncoderViT,\n",
    "        prompt_encoder: PromptEncoder,\n",
    "        mask_decoder: MaskDecoder,\n",
    "        pixel_mean: List[float] = [123.675, 116.28, 103.53],\n",
    "        pixel_std: List[float] = [58.395, 57.12, 57.375],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        SAM predicts object masks from an image and input prompts.\n",
    "\n",
    "        Arguments:\n",
    "          image_encoder (ImageEncoderViT): The backbone used to encode the\n",
    "            image into image embeddings that allow for efficient mask prediction.\n",
    "          prompt_encoder (PromptEncoder): Encodes various types of input prompts.\n",
    "          mask_decoder (MaskDecoder): Predicts masks from the image embeddings\n",
    "            and encoded prompts.\n",
    "          pixel_mean (list(float)): Mean values for normalizing pixels in the input image.\n",
    "          pixel_std (list(float)): Std values for normalizing pixels in the input image.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.register_buffer(\"pixel_mean\", torch.Tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        self.register_buffer(\"pixel_std\", torch.Tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> Any:\n",
    "        return self.pixel_mean.device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        batched_input: List[Dict[str, Any]],\n",
    "        multimask_output: bool,\n",
    "    ) -> List[Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Predicts masks end-to-end from provided images and prompts.\n",
    "        If prompts are not known in advance, using SamPredictor is\n",
    "        recommended over calling the model directly.\n",
    "\n",
    "        Arguments:\n",
    "          batched_input (list(dict)): A list over input images, each a\n",
    "            dictionary with the following keys. A prompt key can be\n",
    "            excluded if it is not present.\n",
    "              'image': The image as a torch tensor in 3xHxW format,\n",
    "                already transformed for input to the model.\n",
    "              'original_size': (tuple(int, int)) The original size of\n",
    "                the image before transformation, as (H, W).\n",
    "              'point_coords': (torch.Tensor) Batched point prompts for\n",
    "                this image, with shape BxNx2. Already transformed to the\n",
    "                input frame of the model.\n",
    "              'point_labels': (torch.Tensor) Batched labels for point prompts,\n",
    "                with shape BxN.\n",
    "              'boxes': (torch.Tensor) Batched box inputs, with shape Bx4.\n",
    "                Already transformed to the input frame of the model.\n",
    "              'mask_inputs': (torch.Tensor) Batched mask inputs to the model,\n",
    "                in the form Bx1xHxW.\n",
    "          multimask_output (bool): Whether the model should predict multiple\n",
    "            disambiguating masks, or return a single mask.\n",
    "\n",
    "        Returns:\n",
    "          (list(dict)): A list over input images, where each element is\n",
    "            as dictionary with the following keys.\n",
    "              'masks': (torch.Tensor) Batched binary mask predictions,\n",
    "                with shape BxCxHxW, where B is the number of input prompts,\n",
    "                C is determined by multimask_output, and (H, W) is the\n",
    "                original size of the image.\n",
    "              'iou_predictions': (torch.Tensor) The model's predictions\n",
    "                of mask quality, in shape BxC.\n",
    "              'low_res_logits': (torch.Tensor) Low resolution logits with\n",
    "                shape BxCxHxW, where H=W=256. Can be passed as mask input\n",
    "                to subsequent iterations of prediction.\n",
    "        \"\"\"\n",
    "        input_images = torch.stack([self.preprocess(x[\"image\"]) for x in batched_input], dim=0)\n",
    "        image_embeddings = self.image_encoder(input_images)\n",
    "\n",
    "        outputs = []\n",
    "        for image_record, curr_embedding in zip(batched_input, image_embeddings):\n",
    "            if \"point_coords\" in image_record:\n",
    "                points = (image_record[\"point_coords\"], image_record[\"point_labels\"])\n",
    "            else:\n",
    "                points = None\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=points,\n",
    "                boxes=image_record.get(\"boxes\", None),\n",
    "                masks=image_record.get(\"mask_inputs\", None),\n",
    "            )\n",
    "            low_res_masks, iou_predictions = self.mask_decoder(\n",
    "                image_embeddings=curr_embedding.unsqueeze(0),\n",
    "                image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=multimask_output,\n",
    "            )\n",
    "            masks = self.postprocess_masks(\n",
    "                low_res_masks,\n",
    "                input_size=image_record[\"image\"].shape[-2:],\n",
    "                original_size=image_record[\"original_size\"],\n",
    "            )\n",
    "            masks = masks > self.mask_threshold\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"masks\": masks,\n",
    "                    \"iou_predictions\": iou_predictions,\n",
    "                    \"low_res_logits\": low_res_masks,\n",
    "                }\n",
    "            )\n",
    "        return outputs\n",
    "\n",
    "    def postprocess_masks(\n",
    "        self,\n",
    "        masks: torch.Tensor,\n",
    "        input_size: Tuple[int, ...],\n",
    "        original_size: Tuple[int, ...],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Remove padding and upscale masks to the original image size.\n",
    "\n",
    "        Arguments:\n",
    "          masks (torch.Tensor): Batched masks from the mask_decoder,\n",
    "            in BxCxHxW format.\n",
    "          input_size (tuple(int, int)): The size of the image input to the\n",
    "            model, in (H, W) format. Used to remove padding.\n",
    "          original_size (tuple(int, int)): The original size of the image\n",
    "            before resizing for input to the model, in (H, W) format.\n",
    "\n",
    "        Returns:\n",
    "          (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n",
    "            is given by original_size.\n",
    "        \"\"\"\n",
    "        masks = F.interpolate(\n",
    "            masks,\n",
    "            (self.image_encoder.img_size, self.image_encoder.img_size),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        masks = masks[..., : input_size[0], : input_size[1]]\n",
    "        masks = F.interpolate(masks, original_size, mode=\"bilinear\", align_corners=False)\n",
    "        return masks\n",
    "\n",
    "    def preprocess(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n",
    "        # Normalize colors\n",
    "        x = (x - self.pixel_mean) / self.pixel_std\n",
    "\n",
    "        # Pad\n",
    "        h, w = x.shape[-2:]\n",
    "        padh = self.image_encoder.img_size - h\n",
    "        padw = self.image_encoder.img_size - w\n",
    "        x = F.pad(x, (0, padw, 0, padh))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63efb381-98c6-4d9b-a3b6-bd11c7fa8c41",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PromptEncoder.__init__() missing 4 required positional arguments: 'embed_dim', 'image_embedding_size', 'input_image_size', and 'mask_in_chans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# model 초기화\u001b[39;00m\n\u001b[0;32m      2\u001b[0m m1 \u001b[39m=\u001b[39m ImageEncoderViT()\n\u001b[1;32m----> 3\u001b[0m m2 \u001b[39m=\u001b[39m PromptEncoder()\n\u001b[0;32m      4\u001b[0m m3 \u001b[39m=\u001b[39m MaskDecoder()\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m Sam(m1, m2, m3)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: PromptEncoder.__init__() missing 4 required positional arguments: 'embed_dim', 'image_embedding_size', 'input_image_size', and 'mask_in_chans'"
     ]
    }
   ],
   "source": [
    "# model 초기화\n",
    "m1 = ImageEncoderViT()\n",
    "m2 = PromptEncoder()\n",
    "m3 = MaskDecoder()\n",
    "model = Sam(m1, m2, m3).to(device)\n",
    "\n",
    "# loss function과 optimizer 정의\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(10):  # 10 에폭 동안 학습합니다.\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        # print(type(outputs))\n",
    "        # print(outputs[0].shape)\n",
    "        # print(outputs[1].shape)\n",
    "        # print(masks.unsqueeze(1).shape)\n",
    "        # print(outputs[0])\n",
    "        L = outputs[0] + 0.4 * outputs[1]\n",
    "        loss = criterion(L, masks.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SatelliteDataset(csv_file='./test.csv', transform=transform, infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3790/3790 [09:15<00:00,  6.83it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    for images in tqdm(test_dataloader):\n",
    "        images = images.float().to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "        masks = np.squeeze(masks, axis=1)\n",
    "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            mask_rle = rle_encode(masks[i])\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2cbbb-04f1-4f9c-b4df-4b744dfce046",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6543d00-32b3-4f2d-a572-d0879fd0a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['mask_rle'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10cb6f-0826-4755-a376-97b695ae8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb78aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a040db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
